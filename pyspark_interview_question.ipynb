{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a95cd465-6116-4767-96b7-3644ab0e54dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Spark\\\\spark-3.3.1-bin-hadoop2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2a427b2-2968-4367-aa53-43e49782ada4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6f74036-662d-4e3e-a381-2c3d4e9bd091",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69bdd4fd-ebad-46b8-a541-243dea973b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Interview_Questions\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87d26fd9-7ddf-43c0-92e6-d62753821395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://Mohammad-Raihan:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Interview_Questions</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1c6b8875570>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a56937f-1a9c-4f31-8083-8b6874021da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+------+\n",
      "| id|   name|age|salary|\n",
      "+---+-------+---+------+\n",
      "|  1|  Alice| 25| 50000|\n",
      "|  2|    Bob| 35| 60000|\n",
      "|  3|Charlie| 28| 55000|\n",
      "|  4|  David| 40| 70000|\n",
      "|  5|    Eve| 22| 48000|\n",
      "+---+-------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Create a Spark session\n",
    "\n",
    "# Sample data for the DataFrame\n",
    "data = [\n",
    "    (1, \"Alice\", 25, 50000),\n",
    "    (2, \"Bob\", 35, 60000),\n",
    "    (3, \"Charlie\", 28, 55000),\n",
    "    (4, \"David\", 40, 70000),\n",
    "    (5, \"Eve\", 22, 48000)\n",
    "]\n",
    "\n",
    "# Create a DataFrame with the given schema\n",
    "columns = [\"id\", \"name\", \"age\", \"salary\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Show the created DataFrame\n",
    "df.show()\n",
    "\n",
    "# Expected Output:\n",
    "# +---+-------+---+------+\n",
    "# | id|   name|age|salary|\n",
    "# +---+-------+---+------+\n",
    "# |  1|  Alice| 25| 50000|\n",
    "# |  2|    Bob| 35| 60000|\n",
    "# |  3|Charlie| 28| 55000|\n",
    "# |  4|  David| 40| 70000|\n",
    "# |  5|    Eve| 22| 48000|\n",
    "# +---+-------+---+------+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "59cafc05-63e5-4ff8-97ff-c20987cb646b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|   name|salary|\n",
      "+-------+------+\n",
      "|  Alice| 50000|\n",
      "|    Bob| 60000|\n",
      "|Charlie| 55000|\n",
      "|  David| 70000|\n",
      "|    Eve| 48000|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select only the name and salary columns.\n",
    "\n",
    "df.select(col('name'),col('salary')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de2dd1f6-4346-408b-884c-97b571d0d95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|   name|   name|\n",
      "+-------+-------+\n",
      "|  Alice|  Alice|\n",
      "|    Bob|    Bob|\n",
      "|Charlie|Charlie|\n",
      "|  David|  David|\n",
      "|    Eve|    Eve|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"name\",\"name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8ac9066-9d2f-42c5-b67b-e2af7d38ae32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|   name|salary|\n",
      "+-------+------+\n",
      "|  Alice| 50000|\n",
      "|    Bob| 60000|\n",
      "|Charlie| 55000|\n",
      "|  David| 70000|\n",
      "|    Eve| 48000|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('name','salary').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94309f18-0b9f-4a49-8da3-25e59a10e8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+------+\n",
      "| id| name|age|salary|\n",
      "+---+-----+---+------+\n",
      "|  2|  Bob| 35| 60000|\n",
      "|  4|David| 40| 70000|\n",
      "+---+-----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter the DataFrame to only include rows where age is greater than 30.\n",
    "df.filter(col('age') >30).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "230a856b-caa3-46a7-bed4-558c6e9c24ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+------+\n",
      "| id| name|age|salary|\n",
      "+---+-----+---+------+\n",
      "|  2|  Bob| 35| 60000|\n",
      "|  4|David| 40| 70000|\n",
      "+---+-----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.age >30).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f5032a2-cacf-4327-9997-0bdbead8b276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+------+\n",
      "| id| name|age|salary|\n",
      "+---+-----+---+------+\n",
      "|  2|  Bob| 35| 60000|\n",
      "|  4|David| 40| 70000|\n",
      "+---+-----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Correct syntax using 'where'\n",
    "df.where(col(\"age\") > 30).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15322940-c853-430f-a34a-48a6743bdd70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: long (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- quantity: long (nullable = true)\n",
      " |-- revenue: double (nullable = true)\n",
      "\n",
      "+----------+----------+--------+-------+\n",
      "|product_id|      date|quantity|revenue|\n",
      "+----------+----------+--------+-------+\n",
      "|         1|2022-01-01|      10|  100.0|\n",
      "|         2|2022-01-02|      20|  300.0|\n",
      "|         1|2022-01-03|      15|  150.0|\n",
      "|         3|2022-01-04|      25|  500.0|\n",
      "|         2|2022-01-05|      30|  600.0|\n",
      "+----------+----------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "\n",
    "# Sample data for the DataFrame\n",
    "data = [\n",
    "    (1, \"2022-01-01\", 10, 100.0),\n",
    "    (2, \"2022-01-02\", 20, 300.0),\n",
    "    (1, \"2022-01-03\", 15, 150.0),\n",
    "    (3, \"2022-01-04\", 25, 500.0),\n",
    "    (2, \"2022-01-05\", 30, 600.0),\n",
    "]\n",
    "\n",
    "# Create a DataFrame with the given schema\n",
    "columns = [\"product_id\", \"date\", \"quantity\", \"revenue\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Convert the 'date' column to a proper date format\n",
    "df = df.withColumn(\"date\", to_date(df[\"date\"], \"yyyy-MM-dd\"))\n",
    "\n",
    "# Show the created \n",
    "df.printSchema()\n",
    "df.show()\n",
    "\n",
    "# Expected output:\n",
    "# +-----------+----------+--------+--------+\n",
    "# | product_id|      date|quantity| revenue|\n",
    "# +-----------+----------+--------+--------+\n",
    "# |          1|2022-01-01|      10|   100.0|\n",
    "# |          2|2022-01-02|      20|   300.0|\n",
    "# |          1|2022-01-03|      15|   150.0|\n",
    "# |          3|2022-01-04|      25|   500.0|\n",
    "# |          2|2022-01-05|      30|   600.0|\n",
    "# +-----------+----------+--------+--------+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93076a26-f031-45dd-b692-2ad0e0db9563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+-------------+\n",
      "|product_id|total_quantity|total_revenue|\n",
      "+----------+--------------+-------------+\n",
      "|         1|            25|        250.0|\n",
      "|         2|            50|        900.0|\n",
      "|         3|            25|        500.0|\n",
      "+----------+--------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group the data by product_id and calculate the total quantity and revenue for each product.\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "grouped_df = df.groupBy('product_id').agg(sum('quantity').alias('total_quantity'),sum('revenue').alias('total_revenue') )\n",
    "grouped_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c48540fa-e12e-48ac-806f-f3bda906c251",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"sales_view\")\n",
    "\n",
    "\n",
    "# Using Spark SQL to query the temporary view\n",
    "query = \"SELECT product_id,sum(quantity),sum(revenue) FROM sales_view group by product_id\"\n",
    "result = spark.sql(query)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7095f8d-a0d4-4822-ba50-a850f6a50f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+------------+\n",
      "|product_id|sum(quantity)|sum(revenue)|\n",
      "+----------+-------------+------------+\n",
      "|         1|           25|       250.0|\n",
      "|         2|           50|       900.0|\n",
      "|         3|           25|       500.0|\n",
      "+----------+-------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34f86e81-7034-41b9-a73d-1caaca90c307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product with the highest total revenue:\n",
      "Row(product_id=2, total_quantity=50, total_revenue=900.0)\n"
     ]
    }
   ],
   "source": [
    "# Find the product with the highest total revenue.\n",
    "# Get the product with the highest total revenue\n",
    "max_revenue_product = grouped_df.orderBy(\"total_revenue\", ascending=False).first()\n",
    "\n",
    "print(\"Product with the highest total revenue:\")\n",
    "print(max_revenue_product)\n",
    "\n",
    "# Expected output:\n",
    "# Product with the highest total revenue:\n",
    "# Row(product_id=2, total_quantity=50, total_revenue=900.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c93f9ee-c184-4960-a6f1-4e998534b5dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+\n",
      "|product_id|total_revenue|\n",
      "+----------+-------------+\n",
      "|         2|        900.0|\n",
      "+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the product with the highest total revenue.\n",
    "result2 = spark.sql(\"\"\"\n",
    "with cte as \n",
    "                    (select product_id,sum(revenue) as total_revenue from sales_view group by product_id order by total_revenue desc)\n",
    "                    \n",
    "                    select * from cte limit 1\n",
    "                    \"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e571d2f3-67d8-4eb9-9e77-f1a7ae4d6ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Employees DataFrame:\n",
      "+-----------+-------+-------------+\n",
      "|employee_id|   name|department_id|\n",
      "+-----------+-------+-------------+\n",
      "|          1|  Alice|          101|\n",
      "|          2|    Bob|          102|\n",
      "|          3|Charlie|          101|\n",
      "|          4|  David|          103|\n",
      "|          5|    Eve|          102|\n",
      "+-----------+-------+-------------+\n",
      "\n",
      "Departments DataFrame:\n",
      "+-------------+---------------+\n",
      "|department_id|department_name|\n",
      "+-------------+---------------+\n",
      "|          101|        Finance|\n",
      "|          102|      Marketing|\n",
      "|          103|             IT|\n",
      "+-------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create sample data for the 'employees' DataFrame\n",
    "employees_data = [\n",
    "    (1, \"Alice\", 101),\n",
    "    (2, \"Bob\", 102),\n",
    "    (3, \"Charlie\", 101),\n",
    "    (4, \"David\", 103),\n",
    "    (5, \"Eve\", 102)\n",
    "]\n",
    "\n",
    "# Create the 'employees' DataFrame\n",
    "employees_columns = [\"employee_id\", \"name\", \"department_id\"]\n",
    "employees_df = spark.createDataFrame(employees_data, employees_columns)\n",
    "\n",
    "# Create sample data for the 'departments' DataFrame\n",
    "departments_data = [\n",
    "    (101, \"Finance\"),\n",
    "    (102, \"Marketing\"),\n",
    "    (103, \"IT\")\n",
    "]\n",
    "\n",
    "# Create the 'departments' DataFrame\n",
    "departments_columns = [\"department_id\", \"department_name\"]\n",
    "departments_df = spark.createDataFrame(departments_data, departments_columns)\n",
    "\n",
    "# Display both DataFrames to check the initial data\n",
    "print(\"Employees DataFrame:\")\n",
    "employees_df.show()\n",
    "\n",
    "print(\"Departments DataFrame:\")\n",
    "departments_df.show()\n",
    "\n",
    "# Expected output for 'employees':\n",
    "# +------------+-------+-------------+\n",
    "# | employee_id|   name|department_id|\n",
    "# +------------+-------+-------------+\n",
    "# |          1|  Alice|          101|\n",
    "# |          2|    Bob|          102|\n",
    "# |          3|Charlie|          101|\n",
    "# |          4|  David|          103|\n",
    "# |          5|    Eve|          102|\n",
    "# +------------+-------+-------------+\n",
    "\n",
    "# Expected output for 'departments':\n",
    "# +-------------+--------------+\n",
    "# |department_id|department_name|\n",
    "# +-------------+--------------+\n",
    "# |          101|      Finance|\n",
    "# |          102|     Marketing|\n",
    "# |          103|            IT|\n",
    "# +-------------+--------------+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f2a8d2ec-52bb-4769-bd84-64831c817756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+-------+---------------+\n",
      "|department_id|employee_id|   name|department_name|\n",
      "+-------------+-----------+-------+---------------+\n",
      "|          101|          1|  Alice|        Finance|\n",
      "|          101|          3|Charlie|        Finance|\n",
      "|          102|          2|    Bob|      Marketing|\n",
      "|          102|          5|    Eve|      Marketing|\n",
      "|          103|          4|  David|             IT|\n",
      "+-------------+-----------+-------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join these DataFrames to get a new DataFrame with employee_id, name, and department_name.\n",
    "\n",
    "new_df = employees_df.join(departments_df, on='department_id', how='inner')\n",
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea48b3a5-9e17-49a0-a53b-b186b3f61ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+---------------+\n",
      "|employee_id|   name|department_name|\n",
      "+-----------+-------+---------------+\n",
      "|          1|  Alice|        Finance|\n",
      "|          3|Charlie|        Finance|\n",
      "|          2|    Bob|      Marketing|\n",
      "|          5|    Eve|      Marketing|\n",
      "|          4|  David|             IT|\n",
      "+-----------+-------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.select('employee_id','name','department_name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c851542b-2409-4212-8fd3-158c70bbba9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+-------+---------------+\n",
      "|department_id|employee_id|   name|department_name|\n",
      "+-------------+-----------+-------+---------------+\n",
      "|          101|          1|  Alice|        Finance|\n",
      "|          101|          3|Charlie|        Finance|\n",
      "+-------------+-----------+-------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter the result to only include employees in the \"Finance\" department.\n",
    "\n",
    "new_df.filter(col('department_name') == 'Finance').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ad4251-fe75-4c98-881f-fd9c2d4cfde3",
   "metadata": {},
   "source": [
    "##### Creating a New Column\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b1c11cee-4c82-473e-9a90-1d1409ea69c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------+--------------+\n",
      "|order_id|product_id|quantity|price_per_unit|\n",
      "+--------+----------+--------+--------------+\n",
      "|       1|       101|       5|          20.0|\n",
      "|       2|       102|      10|          15.0|\n",
      "|       3|       103|       3|          50.0|\n",
      "|       4|       104|       7|          30.0|\n",
      "|       5|       105|       1|         100.0|\n",
      "+--------+----------+--------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, expr\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"DataFrameCreationExample\").getOrCreate()\n",
    "\n",
    "# Sample data for the DataFrame\n",
    "orders_data = [\n",
    "    (1, 101, 5, 20.0),  # order_id, product_id, quantity, price_per_unit\n",
    "    (2, 102, 10, 15.0),\n",
    "    (3, 103, 3, 50.0),\n",
    "    (4, 104, 7, 30.0),\n",
    "    (5, 105, 1, 100.0)\n",
    "]\n",
    "\n",
    "# Create the DataFrame with the given schema\n",
    "orders_columns = [\"order_id\", \"product_id\", \"quantity\", \"price_per_unit\"]\n",
    "orders_df = spark.createDataFrame(orders_data, orders_columns)\n",
    "\n",
    "# Display the DataFrame to check the initial data\n",
    "orders_df.show()\n",
    "\n",
    "# Expected output:\n",
    "# +--------+-----------+--------+--------------+\n",
    "# |order_id|product_id |quantity|price_per_unit|\n",
    "# +--------+-----------+--------+--------------+\n",
    "# |       1|       101|       5|          20.0|\n",
    "# |       2|       102|      10|          15.0|\n",
    "# |       3|       103|       3|          50.0|\n",
    "# |       4|       104|       7|          30.0|\n",
    "# |       5|       105|       1|         100.0|\n",
    "# +--------+-----------+--------+--------------+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d0ff6d94-30c8-4a51-b32a-b0c279c2d1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------+--------------+------------+\n",
      "|order_id|product_id|quantity|price_per_unit|total_price |\n",
      "+--------+----------+--------+--------------+------------+\n",
      "|       1|       101|       5|          20.0|       100.0|\n",
      "|       2|       102|      10|          15.0|       150.0|\n",
      "|       3|       103|       3|          50.0|       150.0|\n",
      "|       4|       104|       7|          30.0|       210.0|\n",
      "|       5|       105|       1|         100.0|       100.0|\n",
      "+--------+----------+--------+--------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add a new column total_price which is the product of quantity and price_per_unit.\n",
    "\n",
    "orders_df.withColumn(\"total_price \", orders_df.quantity * orders_df.price_per_unit).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6d47276b-4f88-4d5a-8060-46676c8cdbd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------+--------------+-----------+-----------+\n",
      "|order_id|product_id|quantity|price_per_unit|total_price|final_price|\n",
      "+--------+----------+--------+--------------+-----------+-----------+\n",
      "|       1|       101|       5|          20.0|      100.0|      100.0|\n",
      "|       2|       102|      10|          15.0|      150.0|      135.0|\n",
      "|       3|       103|       3|          50.0|      150.0|      135.0|\n",
      "|       4|       104|       7|          30.0|      210.0|      189.0|\n",
      "|       5|       105|       1|         100.0|      100.0|      100.0|\n",
      "+--------+----------+--------+--------------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply a discount of 10% if the total_price is greater than 100, and create a new column final_price with the discounted price.\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Create the 'total_price' column\n",
    "orders_df = orders_df.withColumn(\"total_price\",col(\"quantity\") * col(\"price_per_unit\"))\n",
    "\n",
    "# Create the 'final_price' column with a conditional discount\n",
    "orders_df = orders_df.withColumn(\n",
    "    \"final_price\",\n",
    "    when(col(\"total_price\") > 100, col(\"total_price\") * 0.90).otherwise(col(\"total_price\"))\n",
    ")\n",
    "\n",
    "# Display the DataFrame to see the final result\n",
    "orders_df.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c993f5af-85f1-4076-bc19-13a1ab1dbfa3",
   "metadata": {},
   "source": [
    "##### Working with Dates\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "32268d79-04d0-4e7d-ac4e-7924920bdf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the event_date column to a date type.\n",
    "# Add a new column year to represent the year of each event.\n",
    "# Count the number of events for each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f4c8ebf-67ce-4c89-a060-c2bdf3497ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- event_id: long (nullable = true)\n",
      " |-- event_name: string (nullable = true)\n",
      " |-- event_date: date (nullable = true)\n",
      "\n",
      "+--------+---------------+----------+\n",
      "|event_id|     event_name|event_date|\n",
      "+--------+---------------+----------+\n",
      "|       1|  Music Concert|2022-05-14|\n",
      "|       2| Art Exhibition|2023-02-10|\n",
      "|       3|Tech Conference|2022-11-20|\n",
      "|       4|  Food Festival|2023-07-23|\n",
      "|       5|  Film Premiere|2022-09-05|\n",
      "+--------+---------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "\n",
    "\n",
    "# Sample data for the DataFrame\n",
    "events_data = [\n",
    "    (1, \"Music Concert\", \"2022-05-14\"),\n",
    "    (2, \"Art Exhibition\", \"2023-02-10\"),\n",
    "    (3, \"Tech Conference\", \"2022-11-20\"),\n",
    "    (4, \"Food Festival\", \"2023-07-23\"),\n",
    "    (5, \"Film Premiere\", \"2022-09-05\"),\n",
    "]\n",
    "\n",
    "# Create the DataFrame with the given schema\n",
    "events_columns = [\"event_id\", \"event_name\", \"event_date\"]\n",
    "events_df = spark.createDataFrame(events_data, events_columns)\n",
    "\n",
    "# Convert the 'event_date' from string to date type\n",
    "events_df = events_df.withColumn(\"event_date\", to_date(events_df[\"event_date\"], \"yyyy-MM-dd\"))\n",
    "\n",
    "# Display the DataFrame to check the initial data\n",
    "events_df.printSchema()\n",
    "events_df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "76e2b214-8f7d-43ec-8628-5b7d07247f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+----------+----+\n",
      "|event_id|     event_name|event_date|year|\n",
      "+--------+---------------+----------+----+\n",
      "|       1|  Music Concert|2022-05-14|2022|\n",
      "|       2| Art Exhibition|2023-02-10|2023|\n",
      "|       3|Tech Conference|2022-11-20|2022|\n",
      "|       4|  Food Festival|2023-07-23|2023|\n",
      "|       5|  Film Premiere|2022-09-05|2022|\n",
      "+--------+---------------+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add a new column year to represent the year of each event.\n",
    "\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "events_df.withColumn(\"year\", F.year(events_df.event_date)).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cd3b09ef-c494-42c5-a8a1-4ebfe5826150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+----------+----+\n",
      "|event_id|     event_name|event_date|year|\n",
      "+--------+---------------+----------+----+\n",
      "|       1|  Music Concert|2022-05-14|2022|\n",
      "|       2| Art Exhibition|2023-02-10|2023|\n",
      "|       3|Tech Conference|2022-11-20|2022|\n",
      "|       4|  Food Festival|2023-07-23|2023|\n",
      "|       5|  Film Premiere|2022-09-05|2022|\n",
      "+--------+---------------+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "events_df2 = events_df.withColumn(\"year\", F.year(events_df.event_date))\n",
    "events_df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "504f89c2-3c2d-4702-a2ca-068cda5ea9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+\n",
      "|year|no_of_events|\n",
      "+----+------------+\n",
      "|2022|           3|\n",
      "|2023|           2|\n",
      "+----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count the number of events for each year.\n",
    "events_df2.groupBy('year').agg(count('event_name').alias('no_of_events')).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1d8c0d-b25f-4085-b632-e510cad1802d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5dfcb22-71fc-44f7-8818-e25ef05533bd",
   "metadata": {},
   "source": [
    "### Advanced PySpark Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e39c3f-5625-4a1c-9e62-3dcb11445f56",
   "metadata": {},
   "source": [
    "##### Window Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a50faafc-f368-4a99-9e06-261fc5df20d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+----------+------+\n",
      "|transaction_id|customer_id|      date|amount|\n",
      "+--------------+-----------+----------+------+\n",
      "|             1|        101|2022-01-01| 100.0|\n",
      "|             2|        101|2022-01-05| 200.0|\n",
      "|             3|        102|2022-01-03| 150.0|\n",
      "|             4|        102|2022-01-10| 300.0|\n",
      "|             5|        101|2022-01-08| 150.0|\n",
      "+--------------+-----------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_date\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "\n",
    "\n",
    "\n",
    "# Define the schema for the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"transaction_id\", IntegerType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"amount\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Create sample data for the DataFrame\n",
    "transactions_data = [\n",
    "    (1, 101, \"2022-01-01\", 100.0),  # transaction_id, customer_id, date, amount\n",
    "    (2, 101, \"2022-01-05\", 200.0),\n",
    "    (3, 102, \"2022-01-03\", 150.0),\n",
    "    (4, 102, \"2022-01-10\", 300.0),\n",
    "    (5, 101, \"2022-01-08\", 150.0),\n",
    "]\n",
    "\n",
    "# Create the DataFrame with the specified schema\n",
    "transactions_df = spark.createDataFrame(transactions_data, schema)\n",
    "\n",
    "# Convert the 'date' column from string to date type\n",
    "transactions_df = transactions_df.withColumn(\"date\", to_date(transactions_df[\"date\"], \"yyyy-MM-dd\"))\n",
    "\n",
    "# Display the DataFrame to check the initial data\n",
    "transactions_df.show()\n",
    "\n",
    "# Expected output:\n",
    "# +--------------+-----------+----------+------+\n",
    "# |transaction_id|customer_id|      date|amount|\n",
    "# +--------------+-----------+----------+------+\n",
    "# |             1|        101|2022-01-01| 100.0|\n",
    "# |             2|        101|2022-01-05| 200.0|\n",
    "# |             3|        102|2022-01-03| 150.0|\n",
    "# |             4|        102|2022-01-10| 300.0|\n",
    "# |             5|        101|2022-01-08| 150.0|\n",
    "# +--------------+-----------+----------+------+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "707af7ee-a971-4ed9-aa93-94a4ae55bf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column total_amount_per_customer that contains the running total of amount for each customer_id, ordered by date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c01a1e25-409a-468d-8cf6-db03611e6c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+----------+------+-------------+\n",
      "|transaction_id|customer_id|      date|amount|running_total|\n",
      "+--------------+-----------+----------+------+-------------+\n",
      "|             1|        101|2022-01-01| 100.0|        100.0|\n",
      "|             2|        101|2022-01-05| 200.0|        300.0|\n",
      "|             5|        101|2022-01-08| 150.0|        450.0|\n",
      "|             3|        102|2022-01-03| 150.0|        150.0|\n",
      "|             4|        102|2022-01-10| 300.0|        450.0|\n",
      "+--------------+-----------+----------+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Define the WindowSpec for partitioning by 'customer_id' and ordering by 'date'\n",
    "window_spec = Window.partitionBy(\"customer_id\").orderBy(\"date\")\n",
    "\n",
    "# Calculate the running total of 'amount' for each 'customer_id'\n",
    "transactions_df2 = transactions_df.withColumn(\n",
    "    \"running_total\",\n",
    "    sum(\"amount\").over(window_spec)\n",
    ")\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "transactions_df2.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7cb9216e-143e-4a2c-ae7d-c08a8603e722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+----------+------+\n",
      "|transaction_id|customer_id|      date|amount|\n",
      "+--------------+-----------+----------+------+\n",
      "|             1|        101|2022-01-01| 100.0|\n",
      "|             2|        101|2022-01-05| 200.0|\n",
      "|             3|        102|2022-01-03| 150.0|\n",
      "|             4|        102|2022-01-10| 300.0|\n",
      "|             5|        101|2022-01-08| 150.0|\n",
      "+--------------+-----------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions_df.createOrReplaceTempView(\"my_query\")\n",
    "transactions_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c3d6e489-0601-4670-ae7c-c262a2ce74af",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = spark.sql(\"select *,sum(amount) over(partition by customer_id order by date rows between unbounded preceding and current row ) as running_sum from my_query\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8d616f90-716d-4228-94cc-958b02bd85ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+----------+------+-----------+\n",
      "|transaction_id|customer_id|      date|amount|running_sum|\n",
      "+--------------+-----------+----------+------+-----------+\n",
      "|             1|        101|2022-01-01| 100.0|      100.0|\n",
      "|             2|        101|2022-01-05| 200.0|      300.0|\n",
      "|             5|        101|2022-01-08| 150.0|      450.0|\n",
      "|             3|        102|2022-01-03| 150.0|      150.0|\n",
      "|             4|        102|2022-01-10| 300.0|      450.0|\n",
      "+--------------+-----------+----------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b0ae0f48-dfcd-4e51-a659-73be02115ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----------------------------+\n",
      "|user_id|name   |activities                  |\n",
      "+-------+-------+----------------------------+\n",
      "|1      |Alice  |[running, swimming, cycling]|\n",
      "|2      |Bob    |[hiking, climbing]          |\n",
      "|3      |Charlie|[running, gym, yoga]        |\n",
      "|4      |David  |[swimming]                  |\n",
      "|5      |Eve    |[yoga, meditation, pilates] |\n",
      "+-------+-------+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "# Sample data for the DataFrame\n",
    "users_data = [\n",
    "    (1, \"Alice\", [\"running\", \"swimming\", \"cycling\"]),\n",
    "    (2, \"Bob\", [\"hiking\", \"climbing\"]),\n",
    "    (3, \"Charlie\", [\"running\", \"gym\", \"yoga\"]),\n",
    "    (4, \"David\", [\"swimming\"]),\n",
    "    (5, \"Eve\", [\"yoga\", \"meditation\", \"pilates\"])\n",
    "]\n",
    "\n",
    "# Create a DataFrame with an array-type column\n",
    "users_columns = [\"user_id\", \"name\", \"activities\"]\n",
    "users_df = spark.createDataFrame(users_data, users_columns)\n",
    "\n",
    "# Display the DataFrame to check the initial data\n",
    "users_df.show(truncate=False)\n",
    "\n",
    "# Expected output:\n",
    "# +-------+-------+--------------------------+\n",
    "# |user_id|name   |activities                |\n",
    "# +-------+-------+--------------------------+\n",
    "# |1      |Alice  |[running, swimming, cycling] |\n",
    "# |2      |Bob    |[hiking, climbing]        |\n",
    "# |3      |Charlie|[running, gym, yoga]      |\n",
    "# |4      |David  |[swimming]                |\n",
    "# |5      |Eve    |[yoga, meditation, pilates]|\n",
    "# +-------+-------+--------------------------+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6bef9453-2d0e-49fb-9b04-91922e131ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------------------+----------+\n",
      "|user_id|   name|          activities|  activity|\n",
      "+-------+-------+--------------------+----------+\n",
      "|      1|  Alice|[running, swimmin...|   running|\n",
      "|      1|  Alice|[running, swimmin...|  swimming|\n",
      "|      1|  Alice|[running, swimmin...|   cycling|\n",
      "|      2|    Bob|  [hiking, climbing]|    hiking|\n",
      "|      2|    Bob|  [hiking, climbing]|  climbing|\n",
      "|      3|Charlie|[running, gym, yoga]|   running|\n",
      "|      3|Charlie|[running, gym, yoga]|       gym|\n",
      "|      3|Charlie|[running, gym, yoga]|      yoga|\n",
      "|      4|  David|          [swimming]|  swimming|\n",
      "|      5|    Eve|[yoga, meditation...|      yoga|\n",
      "|      5|    Eve|[yoga, meditation...|meditation|\n",
      "|      5|    Eve|[yoga, meditation...|   pilates|\n",
      "+-------+-------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_df.withColumn(\"activity\",explode(\"activities\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "37504300-4e69-452f-a473-9e35d113f2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded_df = users_df.withColumn(\"activity\", explode(\"activities\")).drop(\"activities\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a84202fd-ec93-4462-b538-04039ce46644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----------+\n",
      "|user_id|   name|  activity|\n",
      "+-------+-------+----------+\n",
      "|      1|  Alice|   running|\n",
      "|      1|  Alice|  swimming|\n",
      "|      1|  Alice|   cycling|\n",
      "|      2|    Bob|    hiking|\n",
      "|      2|    Bob|  climbing|\n",
      "|      3|Charlie|   running|\n",
      "|      3|Charlie|       gym|\n",
      "|      3|Charlie|      yoga|\n",
      "|      4|  David|  swimming|\n",
      "|      5|    Eve|      yoga|\n",
      "|      5|    Eve|meditation|\n",
      "|      5|    Eve|   pilates|\n",
      "+-------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exploded_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee707d9-2f3d-4637-a10f-b797f062e28c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0444034a-fdc1-4396-b62e-1a1b18117fd2",
   "metadata": {},
   "source": [
    "#### Pivioting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "801ee9da-e418-41ad-8661-cabdb0b44e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-----+\n",
      "|student_id|subject|score|\n",
      "+----------+-------+-----+\n",
      "|         1|   Math|   95|\n",
      "|         1|Science|   88|\n",
      "|         1|English|   92|\n",
      "|         2|   Math|   78|\n",
      "|         2|Science|   85|\n",
      "|         2|English|   81|\n",
      "|         3|   Math|   87|\n",
      "|         3|Science|   92|\n",
      "|         3|English|   89|\n",
      "+----------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "\n",
    "# Sample data for the DataFrame\n",
    "scores_data = [\n",
    "    (1, \"Math\", 95),  # student_id, subject, score\n",
    "    (1, \"Science\", 88),\n",
    "    (1, \"English\", 92),\n",
    "    (2, \"Math\", 78),\n",
    "    (2, \"Science\", 85),\n",
    "    (2, \"English\", 81),\n",
    "    (3, \"Math\", 87),\n",
    "    (3, \"Science\", 92),\n",
    "    (3, \"English\", 89)\n",
    "]\n",
    "\n",
    "# Create the DataFrame with the given schema\n",
    "scores_columns = [\"student_id\", \"subject\", \"score\"]\n",
    "scores_df = spark.createDataFrame(scores_data, scores_columns)\n",
    "\n",
    "# Display the DataFrame to check the initial data\n",
    "scores_df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8d07f153-c237-4cce-8aca-902f922ef01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+----+-------+\n",
      "|student_id|English|Math|Science|\n",
      "+----------+-------+----+-------+\n",
      "|         1|   92.0|95.0|   88.0|\n",
      "|         3|   89.0|87.0|   92.0|\n",
      "|         2|   81.0|78.0|   85.0|\n",
      "+----------+-------+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores_df.groupBy(\"student_id\").pivot(\"subject\").agg(avg(\"score\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e99d52-4ccd-45e2-b8b3-51f55198f7fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da488ce-fe1b-4264-805e-f2ff4da79a0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7f5ced26-98bb-45fa-8648-5bdd6e8099d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame 1:\n",
      "+-----------+-------+----------+\n",
      "|employee_id|   name|department|\n",
      "+-----------+-------+----------+\n",
      "|          1|  Alice|   Finance|\n",
      "|          2|    Bob| Marketing|\n",
      "|          3|Charlie|        IT|\n",
      "|          4|  David|   Finance|\n",
      "|          5|    Eve|        HR|\n",
      "+-----------+-------+----------+\n",
      "\n",
      "DataFrame 2:\n",
      "+-----------+-------+----------+\n",
      "|employee_id|   name|department|\n",
      "+-----------+-------+----------+\n",
      "|          3|Charlie|        IT|\n",
      "|          6|  Frank| Marketing|\n",
      "|          7|  Grace|   Finance|\n",
      "|          8| Hannah|        HR|\n",
      "|          9|    Ian|        IT|\n",
      "+-----------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the first DataFrame (df1)\n",
    "df1_data = [\n",
    "    (1, \"Alice\", \"Finance\"),\n",
    "    (2, \"Bob\", \"Marketing\"),\n",
    "    (3, \"Charlie\", \"IT\"),\n",
    "    (4, \"David\", \"Finance\"),\n",
    "    (5, \"Eve\", \"HR\")\n",
    "]\n",
    "\n",
    "df1_columns = [\"employee_id\", \"name\", \"department\"]\n",
    "df1 = spark.createDataFrame(df1_data, df1_columns)\n",
    "\n",
    "# Create the second DataFrame (df2)\n",
    "df2_data = [\n",
    "    (3, \"Charlie\", \"IT\"),  # This row is a duplicate\n",
    "    (6, \"Frank\", \"Marketing\"),\n",
    "    (7, \"Grace\", \"Finance\"),\n",
    "    (8, \"Hannah\", \"HR\"),\n",
    "    (9, \"Ian\", \"IT\")\n",
    "]\n",
    "\n",
    "df2_columns = [\"employee_id\", \"name\", \"department\"]\n",
    "df2 = spark.createDataFrame(df2_data, df2_columns)\n",
    "\n",
    "# Display both DataFrames to check the initial data\n",
    "print(\"DataFrame 1:\")\n",
    "df1.show()\n",
    "\n",
    "print(\"DataFrame 2:\")\n",
    "df2.show()\n",
    "\n",
    "# Expected output for 'df1':\n",
    "# +------------+-------+-----------+\n",
    "# |employee_id | name  | department|\n",
    "# +------------+-------+-----------+\n",
    "# |          1| Alice |   Finance |\n",
    "# |          2|  Bob  |  Marketing|\n",
    "# |          3|Charlie|        IT |\n",
    "# |          4| David |   Finance |\n",
    "# |          5|  Eve  |        HR |\n",
    "# +------------+-------+-----------+\n",
    "\n",
    "# Expected output for 'df2':\n",
    "# +------------+-------+-----------+\n",
    "# |employee_id | name  | department|\n",
    "# +------------+-------+-----------+\n",
    "# |          3|Charlie|        IT  |\n",
    "# |          6| Frank |  Marketing |\n",
    "# |          7| Grace |   Finance  |\n",
    "# |          8| Hannah|        HR  |\n",
    "# |          9| Ian   |       IT   |\n",
    "# +------------+-------+-----------+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9a6f6991-9b73-4dd8-83b7-ae4d19de9210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Union the DataFrames.\n",
    "# Drop any duplicate rows based on a unique identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "af3c30f8-1dfa-42da-b097-e613b832dad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+----------+\n",
      "|employee_id|   name|department|\n",
      "+-----------+-------+----------+\n",
      "|          1|  Alice|   Finance|\n",
      "|          2|    Bob| Marketing|\n",
      "|          3|Charlie|        IT|\n",
      "|          4|  David|   Finance|\n",
      "|          5|    Eve|        HR|\n",
      "|          3|Charlie|        IT|\n",
      "|          6|  Frank| Marketing|\n",
      "|          7|  Grace|   Finance|\n",
      "|          8| Hannah|        HR|\n",
      "|          9|    Ian|        IT|\n",
      "+-----------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "union_df = df1.union(df2)\n",
    "union_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "034ad5ce-7a8b-4a38-ad6d-bff97e724ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+----------+\n",
      "|employee_id|   name|department|\n",
      "+-----------+-------+----------+\n",
      "|          1|  Alice|   Finance|\n",
      "|          2|    Bob| Marketing|\n",
      "|          3|Charlie|        IT|\n",
      "|          5|    Eve|        HR|\n",
      "|          4|  David|   Finance|\n",
      "|          6|  Frank| Marketing|\n",
      "|          7|  Grace|   Finance|\n",
      "|          8| Hannah|        HR|\n",
      "|          9|    Ian|        IT|\n",
      "+-----------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "union_df.drop_duplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df099461-d383-4899-9485-18448478a7fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d111bb72-5979-4469-95ce-5e7ac73431fe",
   "metadata": {},
   "source": [
    "#### Udfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "31527269-4185-4838-a723-bc325462125f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+\n",
      "|   name|age|gender|\n",
      "+-------+---+------+\n",
      "|  Alice| 25|     F|\n",
      "|    Bob| 17|     M|\n",
      "|Charlie| 30|     M|\n",
      "|  David| 15|     M|\n",
      "|    Eve| 22|     F|\n",
      "+-------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Define the schema for the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"gender\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Sample data for the DataFrame\n",
    "people_data = [\n",
    "    (\"Alice\", 25, \"F\"),\n",
    "    (\"Bob\", 17, \"M\"),\n",
    "    (\"Charlie\", 30, \"M\"),\n",
    "    (\"David\", 15, \"M\"),\n",
    "    (\"Eve\", 22, \"F\")\n",
    "]\n",
    "\n",
    "# Create the DataFrame with the specified schema\n",
    "people_df = spark.createDataFrame(people_data, schema)\n",
    "\n",
    "# Display the DataFrame to check the initial data\n",
    "people_df.show()\n",
    "\n",
    "# Expected output:\n",
    "# +-------+---+------+\n",
    "# |  name |age|gender|\n",
    "# +-------+---+------+\n",
    "# | Alice | 25|   F  |\n",
    "# |  Bob  | 17|   M  |\n",
    "# |Charlie| 30|   M  |\n",
    "# | David | 15|   M  |\n",
    "# |  Eve  | 22|   F  |\n",
    "# +-------+---+------+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "68e7656d-d231-4220-940d-22f0c2e6b020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+--------+\n",
      "|   name|age|gender|is_minor|\n",
      "+-------+---+------+--------+\n",
      "|  Alice| 25|     F|   false|\n",
      "|    Bob| 17|     M|    true|\n",
      "|Charlie| 30|     M|   false|\n",
      "|  David| 15|     M|    true|\n",
      "|    Eve| 22|     F|   false|\n",
      "+-------+---+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import BooleanType\n",
    "\n",
    "\n",
    "\n",
    "# Define a UDF to determine if a person is a minor\n",
    "def is_minor(age):\n",
    "    return age < 18\n",
    "\n",
    "# Register the UDF with the appropriate return type\n",
    "is_minor_udf = udf(is_minor, BooleanType())\n",
    "\n",
    "# Add a new column 'is_minor' using the UDF\n",
    "people_df = people_df.withColumn(\"is_minor\", is_minor_udf(\"age\"))\n",
    "\n",
    "# Show the DataFrame with the new column\n",
    "people_df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b4d07f-1a6f-4e4b-8fc6-f32a4df5f4b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e84b33fe-1716-40ab-954d-6166c18b18be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salary DataFrame:\n",
      "+-----+-------+-----+------+----------+------+\n",
      "|EmpId|EmpName|Mgrid|deptid|  salarydt|salary|\n",
      "+-----+-------+-----+------+----------+------+\n",
      "|  100|    Raj| null|     1|2023-04-01| 50000|\n",
      "|  200| Joanne|  100|     1|2023-04-01|  4000|\n",
      "|  200| Joanne|  100|     1|2023-04-13|  4500|\n",
      "|  200| Joanne|  100|     1|2023-04-14|  4020|\n",
      "+-----+-------+-----+------+----------+------+\n",
      "\n",
      "Department DataFrame:\n",
      "+------+--------+\n",
      "|deptid|deptname|\n",
      "+------+--------+\n",
      "|     1|      IT|\n",
      "|     2|      HR|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "\n",
    "\n",
    "# Employees salary information\n",
    "data_salary = [\n",
    "    (100, \"Raj\", None, 1, \"01-04-23\", 50000),  # Employee ID, Name, Manager ID, Dept ID, Salary Date, Salary\n",
    "    (200, \"Joanne\", 100, 1, \"01-04-23\", 4000),\n",
    "    (200, \"Joanne\", 100, 1, \"13-04-23\", 4500),\n",
    "    (200, \"Joanne\", 100, 1, \"14-04-23\", 4020),\n",
    "]\n",
    "\n",
    "# Schema for the salary DataFrame\n",
    "schema_salary = [\"EmpId\", \"EmpName\", \"Mgrid\", \"deptid\", \"salarydt\", \"salary\"]\n",
    "\n",
    "# Create the salary DataFrame\n",
    "df_salary = spark.createDataFrame(data_salary, schema_salary)\n",
    "\n",
    "# Convert 'salarydt' from string to date type\n",
    "df_salary = df_salary.withColumn(\"salarydt\", to_date(df_salary[\"salarydt\"], \"dd-MM-yy\"))\n",
    "\n",
    "# Department information\n",
    "data_dept = [(1, \"IT\"), (2, \"HR\")]\n",
    "schema_dept = [\"deptid\", \"deptname\"]\n",
    "\n",
    "# Create the department DataFrame\n",
    "df_dept = spark.createDataFrame(data_dept, schema_dept)\n",
    "\n",
    "# Display the DataFrames\n",
    "print(\"Salary DataFrame:\")\n",
    "df_salary.show()\n",
    "\n",
    "print(\"Department DataFrame:\")\n",
    "df_dept.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "27f0d0fc-c70b-4fcf-a8fe-c9674c1ce9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-------+-----+----------+------+--------+\n",
      "|deptid|EmpId|EmpName|Mgrid|  salarydt|salary|deptname|\n",
      "+------+-----+-------+-----+----------+------+--------+\n",
      "|     1|  100|    Raj| null|2023-04-01| 50000|      IT|\n",
      "|     1|  200| Joanne|  100|2023-04-01|  4000|      IT|\n",
      "|     1|  200| Joanne|  100|2023-04-13|  4500|      IT|\n",
      "|     1|  200| Joanne|  100|2023-04-14|  4020|      IT|\n",
      "+------+-----+-------+-----+----------+------+--------+\n",
      "\n",
      "root\n",
      " |-- deptid: long (nullable = true)\n",
      " |-- EmpId: long (nullable = true)\n",
      " |-- EmpName: string (nullable = true)\n",
      " |-- Mgrid: long (nullable = true)\n",
      " |-- salarydt: date (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- deptname: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df6 = df_salary.join(df_dept, on = 'deptid', how = 'inner')\n",
    "df6.show()\n",
    "df6.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e881d828-9658-4445-9c9c-a16daf79e16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-------+-----+----------+------+--------+\n",
      "|deptid|EmpId|EmpName|Mgrid|  salarydt|salary|deptname|\n",
      "+------+-----+-------+-----+----------+------+--------+\n",
      "|     1|  100|    Raj| null|2023-04-01| 50000|      IT|\n",
      "|     1|  200| Joanne|  100|2023-04-01|  4000|      IT|\n",
      "|     1|  200| Joanne|  100|2023-04-13|  4500|      IT|\n",
      "|     1|  200| Joanne|  100|2023-04-14|  4020|      IT|\n",
      "+------+-----+-------+-----+----------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5 = df_salary.join(df_dept, ['deptid'])\n",
    "df5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e25cd657-2051-4e01-a64a-830215274f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df5.alias('a').join(df5.alias('b'), col('a.Mgrid')==col('b.Mgrid'), how = 'left').select(\n",
    "    col('a.deptname'),\n",
    "    col('b.EmpName').alias('Manager_Name'),\n",
    "    col('a.EmpName'),\n",
    "    col('a.salarydt'),\n",
    "    col('a.salary')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f06e0824-38ed-4041-96c0-32d74eeb20e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+-------+----------+------+\n",
      "|deptname|Manager_Name|EmpName|  salarydt|salary|\n",
      "+--------+------------+-------+----------+------+\n",
      "|      IT|        null|    Raj|2023-04-01| 50000|\n",
      "|      IT|      Joanne| Joanne|2023-04-01|  4000|\n",
      "|      IT|      Joanne| Joanne|2023-04-01|  4000|\n",
      "|      IT|      Joanne| Joanne|2023-04-01|  4000|\n",
      "|      IT|      Joanne| Joanne|2023-04-13|  4500|\n",
      "|      IT|      Joanne| Joanne|2023-04-13|  4500|\n",
      "|      IT|      Joanne| Joanne|2023-04-13|  4500|\n",
      "|      IT|      Joanne| Joanne|2023-04-14|  4020|\n",
      "|      IT|      Joanne| Joanne|2023-04-14|  4020|\n",
      "|      IT|      Joanne| Joanne|2023-04-14|  4020|\n",
      "+--------+------------+-------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8269e11b-2b89-4171-8fe5-e0971860f85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bd1b63cd-cb4f-431d-a9a3-038f7d5d4fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df4.groupby('deptname','Manager_Name','EmpName',year('salarydt').alias('year'),date_format('salarydt','MMM').alias('month')).sum('salary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "996ca609-9a42-4b23-b3f1-29dd23745b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+-------+----+-----+-----------+\n",
      "|deptname|Manager_Name|EmpName|year|month|sum(salary)|\n",
      "+--------+------------+-------+----+-----+-----------+\n",
      "|      IT|        null|    Raj|2023|  Apr|      50000|\n",
      "|      IT|      Joanne| Joanne|2023|  Apr|      37560|\n",
      "+--------+------------+-------+----+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "33ef9ed6-d84c-442c-afa7-3da49443ec5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "68614a24-bff9-4170-ad9b-55af51d3a607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------------+----------+------+--------+\n",
      "|deptid|EmpName|Manager_Name|  salarydt|salary|deptname|\n",
      "+------+-------+------------+----------+------+--------+\n",
      "|     1|    Raj|        null|2023-04-01| 50000|      IT|\n",
      "|     1| Joanne|         Raj|2023-04-01|  4000|      IT|\n",
      "|     1| Joanne|         Raj|2023-04-13|  4500|      IT|\n",
      "|     1| Joanne|         Raj|2023-04-14|  4020|      IT|\n",
      "+------+-------+------------+----------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "# Sample DataFrame with given data\n",
    "data = [\n",
    "    (1, 100, \"Raj\", None, \"2023-04-01\", 50000, \"IT\"),\n",
    "    (1, 200, \"Joanne\", 100, \"2023-04-01\", 4000, \"IT\"),\n",
    "    (1, 200, \"Joanne\", 100, \"2023-04-13\", 4500, \"IT\"),\n",
    "    (1, 200, \"Joanne\", 100, \"2023-04-14\", 4020, \"IT\"),\n",
    "]\n",
    "\n",
    "schema = [\"deptid\", \"EmpId\", \"EmpName\", \"Mgrid\", \"salarydt\", \"salary\", \"deptname\"]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Alias the DataFrame for self left join\n",
    "left_df = df.alias(\"left_df\")\n",
    "right_df = df.alias(\"right_df\")\n",
    "\n",
    "# Perform self left join on 'Mgrid' matching 'EmpId'\n",
    "joined_df = left_df.join(\n",
    "    right_df,\n",
    "    F.col(\"left_df.Mgrid\") == F.col(\"right_df.EmpId\"),\n",
    "    how=\"left\"\n",
    ").select(\n",
    "    F.col(\"left_df.deptid\"),\n",
    "    F.col(\"left_df.EmpName\"),\n",
    "    F.col(\"right_df.EmpName\").alias(\"Manager_Name\"),\n",
    "    F.col(\"left_df.salarydt\"),\n",
    "    F.col(\"left_df.salary\"),\n",
    "    F.col(\"left_df.deptname\")\n",
    ")\n",
    "\n",
    "# Show the joined DataFrame\n",
    "joined_df.show()\n",
    "\n",
    "# Expected output:\n",
    "# +-------+-------+-------------+----------+-------+--------+\n",
    "# | deptid|EmpName|Manager_Name | salarydt | salary|deptname|\n",
    "# +-------+-------+-------------+----------+-------+--------+\n",
    "# |     1 |  Raj  |     null    |2023-04-01| 50000 |     IT |\n",
    "# |     1 | Joanne|     Raj     |2023-04-01|  4000 |     IT |\n",
    "# |     1 | Joanne|     Raj     |2023-04-13|  4500 |     IT |\n",
    "# |     1 | Joanne|     Raj     |2023-04-14|  4020 |     IT |\n",
    "# +-------+-------+-------------+----------+-------+--------+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e11d9a-0506-41d8-9bdb-ce24e733f6c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd88361-34b1-43e9-8bd6-a813738cba6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bcc4a91f-b72b-47a8-95c3-c33850bccca2",
   "metadata": {},
   "source": [
    "### Pyspark Window functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0eed29d-d1b8-4927-a530-f50d3cf88b5b",
   "metadata": {},
   "source": [
    "##### row_number()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1f286786-1859-4ca8-a27d-6156d48755ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|James        |Sales     |3000  |\n",
      "|Michael      |Sales     |4600  |\n",
      "|Robert       |Sales     |4100  |\n",
      "|Maria        |Finance   |3000  |\n",
      "|James        |Sales     |3000  |\n",
      "|Scott        |Finance   |3300  |\n",
      "|Jen          |Finance   |3900  |\n",
      "|Jeff         |Marketing |3000  |\n",
      "|Kumar        |Marketing |2000  |\n",
      "|Saif         |Sales     |4100  |\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simpleData = ((\"James\", \"Sales\", 3000), \\\n",
    "    (\"Michael\", \"Sales\", 4600),  \\\n",
    "    (\"Robert\", \"Sales\", 4100),   \\\n",
    "    (\"Maria\", \"Finance\", 3000),  \\\n",
    "    (\"James\", \"Sales\", 3000),    \\\n",
    "    (\"Scott\", \"Finance\", 3300),  \\\n",
    "    (\"Jen\", \"Finance\", 3900),    \\\n",
    "    (\"Jeff\", \"Marketing\", 3000), \\\n",
    "    (\"Kumar\", \"Marketing\", 2000),\\\n",
    "    (\"Saif\", \"Sales\", 4100) \\\n",
    "  )\n",
    " \n",
    "columns= [\"employee_name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f5a041e7-dd28-485e-b5ef-c22934e3ee85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----------+\n",
      "|employee_name|department|salary|row_number|\n",
      "+-------------+----------+------+----------+\n",
      "|        Maria|   Finance|  3000|         1|\n",
      "|        Scott|   Finance|  3300|         2|\n",
      "|          Jen|   Finance|  3900|         3|\n",
      "|        Kumar| Marketing|  2000|         1|\n",
      "|         Jeff| Marketing|  3000|         2|\n",
      "|        James|     Sales|  3000|         1|\n",
      "|        James|     Sales|  3000|         2|\n",
      "|       Robert|     Sales|  4100|         3|\n",
      "|         Saif|     Sales|  4100|         4|\n",
      "|      Michael|     Sales|  4600|         5|\n",
      "+-------------+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, desc\n",
    "\n",
    "window_spec = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
    "\n",
    "df.withColumn(\"row_number\", row_number().over(window_spec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "27c3d6ff-78e3-4145-bdd3-34d421ae3bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----------+\n",
      "|employee_name|department|salary|row_number|\n",
      "+-------------+----------+------+----------+\n",
      "|          Jen|   Finance|  3900|         1|\n",
      "|        Scott|   Finance|  3300|         2|\n",
      "|        Maria|   Finance|  3000|         3|\n",
      "|         Jeff| Marketing|  3000|         1|\n",
      "|        Kumar| Marketing|  2000|         2|\n",
      "|      Michael|     Sales|  4600|         1|\n",
      "|       Robert|     Sales|  4100|         2|\n",
      "|         Saif|     Sales|  4100|         3|\n",
      "|        James|     Sales|  3000|         4|\n",
      "|        James|     Sales|  3000|         5|\n",
      "+-------------+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, desc\n",
    "\n",
    "window_spec = Window.partitionBy(\"department\").orderBy(desc(\"salary\"))\n",
    "\n",
    "df = df.withColumn(\"row_number\", row_number().over(window_spec))\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e33ee0b-e1f5-4605-b947-55d322d3a401",
   "metadata": {},
   "source": [
    "#### rank() Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f26f33fb-a838-424f-8162-69511c836d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rank\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_spec = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
    "\n",
    "\n",
    "\n",
    "df = df.withColumn(\"rank\", rank().over(window_spec))\n",
    "df.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fea5af-730c-4f15-9c5e-199332401116",
   "metadata": {},
   "source": [
    "##### Dense_Rank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f75ba84c-7a95-484e-a1e6-cc15dc68743e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----------+----+----------+\n",
      "|employee_name|department|salary|row_number|rank|dense_rank|\n",
      "+-------------+----------+------+----------+----+----------+\n",
      "|        Maria|   Finance|  3000|         3|   1|         1|\n",
      "|        Scott|   Finance|  3300|         2|   2|         2|\n",
      "|          Jen|   Finance|  3900|         1|   3|         3|\n",
      "|        Kumar| Marketing|  2000|         2|   1|         1|\n",
      "|         Jeff| Marketing|  3000|         1|   2|         2|\n",
      "|        James|     Sales|  3000|         4|   1|         1|\n",
      "|        James|     Sales|  3000|         5|   1|         1|\n",
      "|       Robert|     Sales|  4100|         2|   3|         2|\n",
      "|         Saif|     Sales|  4100|         3|   3|         2|\n",
      "|      Michael|     Sales|  4600|         1|   5|         3|\n",
      "+-------------+----------+------+----------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import dense_rank\n",
    "df.withColumn(\"dense_rank\", dense_rank().over(Window.partitionBy(\"department\").orderBy(\"salary\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e8529a-8287-4487-9b34-b6331d0228e8",
   "metadata": {},
   "source": [
    "##### percent_rank Window Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "cf0a135f-cdf6-4136-9068-dc7b3ca82f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+------------+\n",
      "|employee_name|department|salary|percent_rank|\n",
      "+-------------+----------+------+------------+\n",
      "|        Maria|   Finance|  3000|         0.0|\n",
      "|        Scott|   Finance|  3300|         0.5|\n",
      "|          Jen|   Finance|  3900|         1.0|\n",
      "|        Kumar| Marketing|  2000|         0.0|\n",
      "|         Jeff| Marketing|  3000|         1.0|\n",
      "|        James|     Sales|  3000|         0.0|\n",
      "|        James|     Sales|  3000|         0.0|\n",
      "|       Robert|     Sales|  4100|         0.5|\n",
      "|         Saif|     Sales|  4100|         0.5|\n",
      "|      Michael|     Sales|  4600|         1.0|\n",
      "+-------------+----------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import percent_rank\n",
    "\n",
    "windowSpec = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
    "df_percent_rank = df.withColumn(\"percent_rank\",percent_rank().over(windowSpec))\n",
    "df_percent_rank.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24b3c2c-0c0f-4cd1-ac21-acc5490c2a32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10dbaa00-d9a5-4fce-a594-89904482d0bc",
   "metadata": {},
   "source": [
    "##### ntile Window Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c655e2f7-6cf0-4d4c-984c-5863148cf615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+-----+\n",
      "|employee_name|department|salary|ntile|\n",
      "+-------------+----------+------+-----+\n",
      "|        Maria|   Finance|  3000|    1|\n",
      "|        Scott|   Finance|  3300|    1|\n",
      "|          Jen|   Finance|  3900|    2|\n",
      "|        Kumar| Marketing|  2000|    1|\n",
      "|         Jeff| Marketing|  3000|    2|\n",
      "|        James|     Sales|  3000|    1|\n",
      "|        James|     Sales|  3000|    1|\n",
      "|       Robert|     Sales|  4100|    1|\n",
      "|         Saif|     Sales|  4100|    2|\n",
      "|      Michael|     Sales|  4600|    2|\n",
      "+-------------+----------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import ntile\n",
    "\n",
    "df_ntile = df.withColumn(\"ntile\", ntile(2).over(Window.partitionBy(\"department\").orderBy(\"salary\")))\n",
    "df_ntile.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "da700a80-2a88-47ab-a140-8e4b57d4fde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|James        |Sales     |3000  |\n",
      "|Michael      |Sales     |4600  |\n",
      "|Robert       |Sales     |4100  |\n",
      "|Maria        |Finance   |3000  |\n",
      "|James        |Sales     |3000  |\n",
      "|Scott        |Finance   |3300  |\n",
      "|Jen          |Finance   |3900  |\n",
      "|Jeff         |Marketing |3000  |\n",
      "|Kumar        |Marketing |2000  |\n",
      "|Saif         |Sales     |4100  |\n",
      "+-------------+----------+------+\n",
      "\n",
      "+-------------+----------+------+----------+\n",
      "|employee_name|department|salary|row_number|\n",
      "+-------------+----------+------+----------+\n",
      "|Maria        |Finance   |3000  |1         |\n",
      "|Scott        |Finance   |3300  |2         |\n",
      "|Jen          |Finance   |3900  |3         |\n",
      "|Kumar        |Marketing |2000  |1         |\n",
      "|Jeff         |Marketing |3000  |2         |\n",
      "|James        |Sales     |3000  |1         |\n",
      "|James        |Sales     |3000  |2         |\n",
      "|Robert       |Sales     |4100  |3         |\n",
      "|Saif         |Sales     |4100  |4         |\n",
      "|Michael      |Sales     |4600  |5         |\n",
      "+-------------+----------+------+----------+\n",
      "\n",
      "+-------------+----------+------+----+\n",
      "|employee_name|department|salary|rank|\n",
      "+-------------+----------+------+----+\n",
      "|        Maria|   Finance|  3000|   1|\n",
      "|        Scott|   Finance|  3300|   2|\n",
      "|          Jen|   Finance|  3900|   3|\n",
      "|        Kumar| Marketing|  2000|   1|\n",
      "|         Jeff| Marketing|  3000|   2|\n",
      "|        James|     Sales|  3000|   1|\n",
      "|        James|     Sales|  3000|   1|\n",
      "|       Robert|     Sales|  4100|   3|\n",
      "|         Saif|     Sales|  4100|   3|\n",
      "|      Michael|     Sales|  4600|   5|\n",
      "+-------------+----------+------+----+\n",
      "\n",
      "+-------------+----------+------+----------+\n",
      "|employee_name|department|salary|dense_rank|\n",
      "+-------------+----------+------+----------+\n",
      "|        Maria|   Finance|  3000|         1|\n",
      "|        Scott|   Finance|  3300|         2|\n",
      "|          Jen|   Finance|  3900|         3|\n",
      "|        Kumar| Marketing|  2000|         1|\n",
      "|         Jeff| Marketing|  3000|         2|\n",
      "|        James|     Sales|  3000|         1|\n",
      "|        James|     Sales|  3000|         1|\n",
      "|       Robert|     Sales|  4100|         2|\n",
      "|         Saif|     Sales|  4100|         2|\n",
      "|      Michael|     Sales|  4600|         3|\n",
      "+-------------+----------+------+----------+\n",
      "\n",
      "+-------------+----------+------+------------+\n",
      "|employee_name|department|salary|percent_rank|\n",
      "+-------------+----------+------+------------+\n",
      "|        Maria|   Finance|  3000|         0.0|\n",
      "|        Scott|   Finance|  3300|         0.5|\n",
      "|          Jen|   Finance|  3900|         1.0|\n",
      "|        Kumar| Marketing|  2000|         0.0|\n",
      "|         Jeff| Marketing|  3000|         1.0|\n",
      "|        James|     Sales|  3000|         0.0|\n",
      "|        James|     Sales|  3000|         0.0|\n",
      "|       Robert|     Sales|  4100|         0.5|\n",
      "|         Saif|     Sales|  4100|         0.5|\n",
      "|      Michael|     Sales|  4600|         1.0|\n",
      "+-------------+----------+------+------------+\n",
      "\n",
      "+-------------+----------+------+-----+\n",
      "|employee_name|department|salary|ntile|\n",
      "+-------------+----------+------+-----+\n",
      "|        Maria|   Finance|  3000|    1|\n",
      "|        Scott|   Finance|  3300|    1|\n",
      "|          Jen|   Finance|  3900|    2|\n",
      "|        Kumar| Marketing|  2000|    1|\n",
      "|         Jeff| Marketing|  3000|    2|\n",
      "|        James|     Sales|  3000|    1|\n",
      "|        James|     Sales|  3000|    1|\n",
      "|       Robert|     Sales|  4100|    1|\n",
      "|         Saif|     Sales|  4100|    2|\n",
      "|      Michael|     Sales|  4600|    2|\n",
      "+-------------+----------+------+-----+\n",
      "\n",
      "+-------------+----------+------+------------------+\n",
      "|employee_name|department|salary|         cume_dist|\n",
      "+-------------+----------+------+------------------+\n",
      "|        Maria|   Finance|  3000|0.3333333333333333|\n",
      "|        Scott|   Finance|  3300|0.6666666666666666|\n",
      "|          Jen|   Finance|  3900|               1.0|\n",
      "|        Kumar| Marketing|  2000|               0.5|\n",
      "|         Jeff| Marketing|  3000|               1.0|\n",
      "|        James|     Sales|  3000|               0.4|\n",
      "|        James|     Sales|  3000|               0.4|\n",
      "|       Robert|     Sales|  4100|               0.8|\n",
      "|         Saif|     Sales|  4100|               0.8|\n",
      "|      Michael|     Sales|  4600|               1.0|\n",
      "+-------------+----------+------+------------------+\n",
      "\n",
      "+-------------+----------+------+----+\n",
      "|employee_name|department|salary| lag|\n",
      "+-------------+----------+------+----+\n",
      "|        Maria|   Finance|  3000|null|\n",
      "|        Scott|   Finance|  3300|null|\n",
      "|          Jen|   Finance|  3900|3000|\n",
      "|        Kumar| Marketing|  2000|null|\n",
      "|         Jeff| Marketing|  3000|null|\n",
      "|        James|     Sales|  3000|null|\n",
      "|        James|     Sales|  3000|null|\n",
      "|       Robert|     Sales|  4100|3000|\n",
      "|         Saif|     Sales|  4100|3000|\n",
      "|      Michael|     Sales|  4600|4100|\n",
      "+-------------+----------+------+----+\n",
      "\n",
      "+-------------+----------+------+----+\n",
      "|employee_name|department|salary|lead|\n",
      "+-------------+----------+------+----+\n",
      "|        Maria|   Finance|  3000|3900|\n",
      "|        Scott|   Finance|  3300|null|\n",
      "|          Jen|   Finance|  3900|null|\n",
      "|        Kumar| Marketing|  2000|null|\n",
      "|         Jeff| Marketing|  3000|null|\n",
      "|        James|     Sales|  3000|4100|\n",
      "|        James|     Sales|  3000|4100|\n",
      "|       Robert|     Sales|  4100|4600|\n",
      "|         Saif|     Sales|  4100|null|\n",
      "|      Michael|     Sales|  4600|null|\n",
      "+-------------+----------+------+----+\n",
      "\n",
      "+----------+------+-----+----+----+\n",
      "|department|   avg|  sum| min| max|\n",
      "+----------+------+-----+----+----+\n",
      "|   Finance|3400.0|10200|3000|3900|\n",
      "| Marketing|2500.0| 5000|2000|3000|\n",
      "|     Sales|3760.0|18800|3000|4600|\n",
      "+----------+------+-----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "simpleData = ((\"James\", \"Sales\", 3000), \\\n",
    "    (\"Michael\", \"Sales\", 4600),  \\\n",
    "    (\"Robert\", \"Sales\", 4100),   \\\n",
    "    (\"Maria\", \"Finance\", 3000),  \\\n",
    "    (\"James\", \"Sales\", 3000),    \\\n",
    "    (\"Scott\", \"Finance\", 3300),  \\\n",
    "    (\"Jen\", \"Finance\", 3900),    \\\n",
    "    (\"Jeff\", \"Marketing\", 3000), \\\n",
    "    (\"Kumar\", \"Marketing\", 2000),\\\n",
    "    (\"Saif\", \"Sales\", 4100) \\\n",
    "  )\n",
    " \n",
    "columns= [\"employee_name\", \"department\", \"salary\"]\n",
    "\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "windowSpec  = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
    "\n",
    "df.withColumn(\"row_number\",row_number().over(windowSpec)) \\\n",
    "    .show(truncate=False)\n",
    "\n",
    "from pyspark.sql.functions import rank\n",
    "df.withColumn(\"rank\",rank().over(windowSpec)) \\\n",
    "    .show()\n",
    "\n",
    "from pyspark.sql.functions import dense_rank\n",
    "df.withColumn(\"dense_rank\",dense_rank().over(windowSpec)) \\\n",
    "    .show()\n",
    "\n",
    "from pyspark.sql.functions import percent_rank\n",
    "df.withColumn(\"percent_rank\",percent_rank().over(windowSpec)) \\\n",
    "    .show()\n",
    "    \n",
    "from pyspark.sql.functions import ntile\n",
    "df.withColumn(\"ntile\",ntile(2).over(windowSpec)) \\\n",
    "    .show()\n",
    "\n",
    "from pyspark.sql.functions import cume_dist    \n",
    "df.withColumn(\"cume_dist\",cume_dist().over(windowSpec)) \\\n",
    "   .show()\n",
    " \n",
    "from pyspark.sql.functions import lag    \n",
    "df.withColumn(\"lag\",lag(\"salary\",2).over(windowSpec)) \\\n",
    "      .show()\n",
    "\n",
    "from pyspark.sql.functions import lead    \n",
    "df.withColumn(\"lead\",lead(\"salary\",2).over(windowSpec)) \\\n",
    "    .show()\n",
    "    \n",
    "windowSpecAgg  = Window.partitionBy(\"department\")\n",
    "from pyspark.sql.functions import col,avg,sum,min,max,row_number \n",
    "df.withColumn(\"row\",row_number().over(windowSpec)) \\\n",
    "  .withColumn(\"avg\", avg(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"sum\", sum(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"min\", min(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"max\", max(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .where(col(\"row\")==1).select(\"department\",\"avg\",\"sum\",\"min\",\"max\") \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67ee80a-a2d0-4503-9476-3d470d85780c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199ca282-42aa-4c50-9a82-95fdacf88d79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d160c700-2a71-4a83-8959-74aa03d4e1de",
   "metadata": {},
   "source": [
    "#### Pysaprk Union functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "bd1cd95e-a260-4838-af35-8b3a7da8237e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- bonus: long (nullable = true)\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000), \\\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000), \\\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000) \\\n",
    "  ]\n",
    "\n",
    "columns= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8d6e1f79-4dd9-4835-80bf-27949568fdfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- bonus: long (nullable = true)\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame2\n",
    "simpleData2 = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000), \\\n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000), \\\n",
    "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000), \\\n",
    "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000) \\\n",
    "  ]\n",
    "columns2= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "\n",
    "df2 = spark.createDataFrame(data = simpleData2, schema = columns2)\n",
    "\n",
    "df2.printSchema()\n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e250107b-450d-47d6-ac02-2b7602fcda37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|        James|     Sales|   NY| 90000| 34|10000|\n",
      "|      Michael|     Sales|   NY| 86000| 56|20000|\n",
      "|       Robert|     Sales|   CA| 81000| 30|23000|\n",
      "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
      "|        James|     Sales|   NY| 90000| 34|10000|\n",
      "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
      "|          Jen|   Finance|   NY| 79000| 53|15000|\n",
      "|         Jeff| Marketing|   CA| 80000| 25|18000|\n",
      "|        Kumar| Marketing|   NY| 91000| 50|21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_union = df.union(df2)\n",
    "df_union.show()   ## it will contain only unique records\n",
    "df_union.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "57eabd6f-3a23-4a6b-8859-bb0455dceaf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|        James|     Sales|   NY| 90000| 34|10000|\n",
      "|      Michael|     Sales|   NY| 86000| 56|20000|\n",
      "|       Robert|     Sales|   CA| 81000| 30|23000|\n",
      "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
      "|        James|     Sales|   NY| 90000| 34|10000|\n",
      "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
      "|          Jen|   Finance|   NY| 79000| 53|15000|\n",
      "|         Jeff| Marketing|   CA| 80000| 25|18000|\n",
      "|        Kumar| Marketing|   NY| 91000| 50|21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_union_all = df.unionAll(df2)\n",
    "df_union_all.show()  ## It will contain duplicates records also\n",
    "df_union_all.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5c971405-02aa-4631-a7cc-5d3bb29a556c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Merge without Duplicates\n",
    "\n",
    "# Remove duplicates after union() using distinct()\n",
    "disDF = df.union(df2).distinct()\n",
    "disDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e858ba42-19ac-4ccf-b5bf-9ad75be8fa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "disDF.createOrReplaceTempView(\"my_query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2faec0f1-c2ce-4ed2-ab34-87e6d2a79484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|        James|     Sales|   NY| 90000| 34|10000|\n",
      "|      Michael|     Sales|   NY| 86000| 56|20000|\n",
      "|       Robert|     Sales|   CA| 81000| 30|23000|\n",
      "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
      "|          Jen|   Finance|   NY| 79000| 53|15000|\n",
      "|        Kumar| Marketing|   NY| 91000| 50|21000|\n",
      "|         Jeff| Marketing|   CA| 80000| 25|18000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from my_query\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662f2d1a-d634-4cb1-9f27-0aa8b0c70193",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1534952e-4c25-4fed-8ef1-9d679dea0245",
   "metadata": {},
   "source": [
    "###### Employees who earn more than their managers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8b90b3ff-8a32-4586-b298-1b2338ac7c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "259d0168-a04c-446c-bf54-e3891cf21d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_id: integer (nullable = true)\n",
      " |-- emp_name: string (nullable = true)\n",
      " |-- job_name: string (nullable = true)\n",
      " |-- manager_id: integer (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      " |-- salary: float (nullable = true)\n",
      " |-- commission: float (nullable = true)\n",
      " |-- dep_id: integer (nullable = true)\n",
      "\n",
      "+------+--------+---------+----------+----------+------+----------+------+\n",
      "|emp_id|emp_name| job_name|manager_id| hire_date|salary|commission|dep_id|\n",
      "+------+--------+---------+----------+----------+------+----------+------+\n",
      "| 68319| KAYLING|PRESIDENT|      null|1991-11-18|6000.0|      null|  1001|\n",
      "| 66928|   BLAZE|  MANAGER|     68319|1991-05-01|2750.0|      null|  3001|\n",
      "| 67832|   CLARE|  MANAGER|     68319|1991-06-09|2550.0|      null|  1001|\n",
      "| 65646|   JONAS|  MANAGER|     68319|1991-04-02|2957.0|      null|  2001|\n",
      "| 67858| SCARLET|  ANALYST|     65646|1997-04-19|3100.0|      null|  2001|\n",
      "| 69062|   FRANK|  ANALYST|     65646|1991-12-03|3100.0|      null|  2001|\n",
      "| 63679|SANDRINE|    CLERK|     69062|1990-12-18| 900.0|      null|  2001|\n",
      "| 64989|  ADELYN| SALESMAN|     66928|1991-02-20|1700.0|     400.0|  3001|\n",
      "| 65271|    WADE| SALESMAN|     66928|1991-02-22|1350.0|     600.0|  3001|\n",
      "| 66564|  MADDEN| SALESMAN|     66928|1991-09-28|1350.0|    1500.0|  3001|\n",
      "| 68454|  TUCKER| SALESMAN|     66928|1991-09-08|1600.0|       0.0|  3001|\n",
      "| 68736|  ADNRES|    CLERK|     67858|1997-05-23|1200.0|      null|  2001|\n",
      "| 69000|  JULIUS|    CLERK|     66928|1991-12-03|1050.0|      null|  3001|\n",
      "| 69324|  MARKER|    CLERK|     67832|1992-01-23|1400.0|      null|  1001|\n",
      "+------+--------+---------+----------+----------+------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType, DateType\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "# Convert date strings to datetime.date objects\n",
    "def str_to_date(date_str):\n",
    "    return datetime.strptime(date_str, \"%Y-%m-%d\").date()\n",
    "\n",
    "# Define the data with date conversion\n",
    "data = [\n",
    "    {'emp_id': 68319, 'emp_name': 'KAYLING', 'job_name': 'PRESIDENT', 'manager_id': None, 'hire_date': str_to_date(\"1991-11-18\"), 'salary': 6000.00, 'commission': None, 'dep_id': 1001},\n",
    "    {'emp_id': 66928, 'emp_name': 'BLAZE', 'job_name': 'MANAGER', 'manager_id': 68319, 'hire_date': str_to_date(\"1991-05-01\"), 'salary': 2750.00, 'commission': None, 'dep_id': 3001},\n",
    "    {'emp_id': 67832, 'emp_name': 'CLARE', 'job_name': 'MANAGER', 'manager_id': 68319, 'hire_date': str_to_date(\"1991-06-09\"), 'salary': 2550.00, 'commission': None, 'dep_id': 1001},\n",
    "    {'emp_id': 65646, 'emp_name': 'JONAS', 'job_name': 'MANAGER', 'manager_id': 68319, 'hire_date': str_to_date(\"1991-04-02\"), 'salary': 2957.00, 'commission': None, 'dep_id': 2001},\n",
    "    {'emp_id': 67858, 'emp_name': 'SCARLET', 'job_name': 'ANALYST', 'manager_id': 65646, 'hire_date': str_to_date(\"1997-04-19\"), 'salary': 3100.00, 'commission': None, 'dep_id': 2001},\n",
    "    {'emp_id': 69062, 'emp_name': 'FRANK', 'job_name': 'ANALYST', 'manager_id': 65646, 'hire_date': str_to_date(\"1991-12-03\"), 'salary': 3100.00, 'commission': None, 'dep_id': 2001},\n",
    "    {'emp_id': 63679, 'emp_name': 'SANDRINE', 'job_name': 'CLERK', 'manager_id': 69062, 'hire_date': str_to_date(\"1990-12-18\"), 'salary': 900.00, 'commission': None, 'dep_id': 2001},\n",
    "    {'emp_id': 64989, 'emp_name': 'ADELYN', 'job_name': 'SALESMAN', 'manager_id': 66928, 'hire_date': str_to_date(\"1991-02-20\"), 'salary': 1700.00, 'commission': 400.00, 'dep_id': 3001},\n",
    "    {'emp_id': 65271, 'emp_name': 'WADE', 'job_name': 'SALESMAN', 'manager_id': 66928, 'hire_date': str_to_date(\"1991-02-22\"), 'salary': 1350.00, 'commission': 600.00, 'dep_id': 3001},\n",
    "    {'emp_id': 66564, 'emp_name': 'MADDEN', 'job_name': 'SALESMAN', 'manager_id': 66928, 'hire_date': str_to_date(\"1991-09-28\"), 'salary': 1350.00, 'commission': 1500.00, 'dep_id': 3001},\n",
    "    {'emp_id': 68454, 'emp_name': 'TUCKER', 'job_name': 'SALESMAN', 'manager_id': 66928, 'hire_date': str_to_date(\"1991-09-08\"), 'salary': 1600.00, 'commission': 0.00, 'dep_id': 3001},\n",
    "    {'emp_id': 68736, 'emp_name': 'ADNRES', 'job_name': 'CLERK', 'manager_id': 67858, 'hire_date': str_to_date(\"1997-05-23\"), 'salary': 1200.00, 'commission': None, 'dep_id': 2001},\n",
    "    {'emp_id': 69000, 'emp_name': 'JULIUS', 'job_name': 'CLERK', 'manager_id': 66928, 'hire_date': str_to_date(\"1991-12-03\"), 'salary': 1050.00, 'commission': None, 'dep_id': 3001},\n",
    "    {'emp_id': 69324, 'emp_name': 'MARKER', 'job_name': 'CLERK', 'manager_id': 67832, 'hire_date': str_to_date(\"1992-01-23\"), 'salary': 1400.00, 'commission': None, 'dep_id': 1001}\n",
    "]\n",
    "\n",
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"emp_id\", IntegerType(), True),\n",
    "    StructField(\"emp_name\", StringType(), True),\n",
    "    StructField(\"job_name\", StringType(), True),\n",
    "    StructField(\"manager_id\", IntegerType(), True),\n",
    "    StructField(\"hire_date\", DateType(), True),\n",
    "    StructField(\"salary\", FloatType(), True),\n",
    "    StructField(\"commission\", FloatType(), True),\n",
    "    StructField(\"dep_id\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Create the DataFrame\n",
    "df_employee = spark.createDataFrame(data, schema)\n",
    "\n",
    "df_employee.printSchema()\n",
    "df_employee.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "11504d4a-ba10-46ec-aab7-e44d9ba2a597",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_employee.createOrReplaceTempView(\"my_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "321024f4-6b8b-4415-b64d-93151ff302f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------+----------+----------+------+----------+------+\n",
      "|emp_id|emp_name| job_name|manager_id| hire_date|salary|commission|dep_id|\n",
      "+------+--------+---------+----------+----------+------+----------+------+\n",
      "| 68319| KAYLING|PRESIDENT|      null|1991-11-18|6000.0|      null|  1001|\n",
      "| 66928|   BLAZE|  MANAGER|     68319|1991-05-01|2750.0|      null|  3001|\n",
      "| 67832|   CLARE|  MANAGER|     68319|1991-06-09|2550.0|      null|  1001|\n",
      "| 65646|   JONAS|  MANAGER|     68319|1991-04-02|2957.0|      null|  2001|\n",
      "| 67858| SCARLET|  ANALYST|     65646|1997-04-19|3100.0|      null|  2001|\n",
      "| 69062|   FRANK|  ANALYST|     65646|1991-12-03|3100.0|      null|  2001|\n",
      "| 63679|SANDRINE|    CLERK|     69062|1990-12-18| 900.0|      null|  2001|\n",
      "| 64989|  ADELYN| SALESMAN|     66928|1991-02-20|1700.0|     400.0|  3001|\n",
      "| 65271|    WADE| SALESMAN|     66928|1991-02-22|1350.0|     600.0|  3001|\n",
      "| 66564|  MADDEN| SALESMAN|     66928|1991-09-28|1350.0|    1500.0|  3001|\n",
      "| 68454|  TUCKER| SALESMAN|     66928|1991-09-08|1600.0|       0.0|  3001|\n",
      "| 68736|  ADNRES|    CLERK|     67858|1997-05-23|1200.0|      null|  2001|\n",
      "| 69000|  JULIUS|    CLERK|     66928|1991-12-03|1050.0|      null|  3001|\n",
      "| 69324|  MARKER|    CLERK|     67832|1992-01-23|1400.0|      null|  1001|\n",
      "+------+--------+---------+----------+----------+------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from my_table\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8ae8aebd-ef84-4b86-b0cd-aa69528cef52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+---------------+--------------+\n",
      "|employee_name|manager_name|employee_salary|manager_salary|\n",
      "+-------------+------------+---------------+--------------+\n",
      "|      SCARLET|       JONAS|         3100.0|        2957.0|\n",
      "|        FRANK|       JONAS|         3100.0|        2957.0|\n",
      "+-------------+------------+---------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select t1.emp_name as employee_name, t2.emp_name as manager_name, t1.salary as employee_salary, t2.salary as manager_salary from my_table t1\n",
    "inner join my_table t2 on t1.manager_id = t2.emp_id\n",
    "where t1.salary>t2.salary\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "218b6316-ad8f-42b6-8bc0-ddc307c2b414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------+----------+----------+------+----------+------+\n",
      "|emp_id|emp_name| job_name|manager_id| hire_date|salary|commission|dep_id|\n",
      "+------+--------+---------+----------+----------+------+----------+------+\n",
      "| 68319| KAYLING|PRESIDENT|      null|1991-11-18|6000.0|      null|  1001|\n",
      "| 66928|   BLAZE|  MANAGER|     68319|1991-05-01|2750.0|      null|  3001|\n",
      "| 67832|   CLARE|  MANAGER|     68319|1991-06-09|2550.0|      null|  1001|\n",
      "| 65646|   JONAS|  MANAGER|     68319|1991-04-02|2957.0|      null|  2001|\n",
      "| 67858| SCARLET|  ANALYST|     65646|1997-04-19|3100.0|      null|  2001|\n",
      "| 69062|   FRANK|  ANALYST|     65646|1991-12-03|3100.0|      null|  2001|\n",
      "| 63679|SANDRINE|    CLERK|     69062|1990-12-18| 900.0|      null|  2001|\n",
      "| 64989|  ADELYN| SALESMAN|     66928|1991-02-20|1700.0|     400.0|  3001|\n",
      "| 65271|    WADE| SALESMAN|     66928|1991-02-22|1350.0|     600.0|  3001|\n",
      "| 66564|  MADDEN| SALESMAN|     66928|1991-09-28|1350.0|    1500.0|  3001|\n",
      "| 68454|  TUCKER| SALESMAN|     66928|1991-09-08|1600.0|       0.0|  3001|\n",
      "| 68736|  ADNRES|    CLERK|     67858|1997-05-23|1200.0|      null|  2001|\n",
      "| 69000|  JULIUS|    CLERK|     66928|1991-12-03|1050.0|      null|  3001|\n",
      "| 69324|  MARKER|    CLERK|     67832|1992-01-23|1400.0|      null|  1001|\n",
      "+------+--------+---------+----------+----------+------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_employee_2 = df_employee\n",
    "df_employee_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d9353a-9cf1-45b6-a49e-10dcbd30d012",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6a38ae-e6cb-42a4-9ae1-8673188e3b98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f4a9b6e-4e0a-4700-80d4-065d3ed3d88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- input_timestamp: string (nullable = true)\n",
      "\n",
      "+---+-----------------------+\n",
      "|id |input_timestamp        |\n",
      "+---+-----------------------+\n",
      "|1  |2019-06-24 12:01:19.000|\n",
      "+---+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.createDataFrame(\n",
    "        data = [ (\"1\",\"2019-06-24 12:01:19.000\")],\n",
    "        schema=[\"id\",\"input_timestamp\"])\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3b8249a-7a1f-49c4-b998-0bb7c1081c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee8cb590-d015-4f90-9383-eb108612b5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------+----------+\n",
      "|id |input_timestamp        |date_type |\n",
      "+---+-----------------------+----------+\n",
      "|1  |2019-06-24 12:01:19.000|2019-06-24|\n",
      "+---+-----------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"date_type\", to_date(\"input_timestamp\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "518616a1-46cc-44e7-9f6d-0bec1d96a8e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------+----------+\n",
      "|id |input_timestamp        |date_type |\n",
      "+---+-----------------------+----------+\n",
      "|1  |2019-06-24 12:01:19.000|2024-05-10|\n",
      "+---+-----------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"date_type\", to_date(current_timestamp())).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7867786e-acfb-4046-b674-d801656f7ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------+\n",
      "|to_date(05-10-2024 12:01:19.000, MM-dd-yyyy HH:mm:ss.SSSS)|\n",
      "+----------------------------------------------------------+\n",
      "|                                                2024-05-10|\n",
      "+----------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Custom Timestamp format to DateType\n",
    "df.select(to_date(lit('05-10-2024 12:01:19.000'),'MM-dd-yyyy HH:mm:ss.SSSS')) \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5050bdb-64a7-404d-bc24-a04ab77966e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------+-------------------+----------+\n",
      "|id |input_timestamp        |ts                 |datetype  |\n",
      "+---+-----------------------+-------------------+----------+\n",
      "|1  |2019-06-24 12:01:19.000|2019-06-24 12:01:19|2019-06-24|\n",
      "+---+-----------------------+-------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Timestamp type to DateType\n",
    "df.withColumn(\"ts\",to_timestamp(col(\"input_timestamp\"))) \\\n",
    "  .withColumn(\"datetype\",to_date(col(\"ts\"))) \\\n",
    "  .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "22de3b6b-89a4-400b-ad35-7b2a0bb7005d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------+----------+\n",
      "|id |input_timestamp        |date_type |\n",
      "+---+-----------------------+----------+\n",
      "|1  |2019-06-24 12:01:19.000|2019-06-24|\n",
      "+---+-----------------------+----------+\n",
      "\n",
      "+---+-----------------------+----------+\n",
      "|id |input_timestamp        |date_type |\n",
      "+---+-----------------------+----------+\n",
      "|1  |2019-06-24 12:01:19.000|2019-06-24|\n",
      "+---+-----------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using Cast to convert Timestamp String to DateType\n",
    "df.withColumn('date_type', col('input_timestamp').cast('date')) \\\n",
    "       .show(truncate=False)\n",
    "\n",
    "# Using Cast to convert TimestampType to DateType\n",
    "df.withColumn('date_type', to_timestamp('input_timestamp').cast('date')) \\\n",
    "  .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a139dcb-1ae2-4d5c-adf4-db63cd5f1b78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7cf017-d0e0-4e20-84d9-b01dbc7944fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4dd23032-ffea-402b-9e4b-bbc47c0d9944",
   "metadata": {},
   "source": [
    "##### Broadcast join "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4766bf9d-6556-448b-83be-476cb0c01b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# Chhota dataframe\n",
    "cities = spark.createDataFrame([\n",
    "    (1, \"New York\"),\n",
    "    (2, \"Los Angeles\"),\n",
    "    (3, \"Chicago\")\n",
    "], [\"city_id\", \"city_name\"])\n",
    "\n",
    "# Bada dataframe\n",
    "people = spark.createDataFrame([\n",
    "    (1, \"John Doe\", 1),\n",
    "    (2, \"Jane Doe\", 2),\n",
    "    (3, \"Sam Smith\", 3),\n",
    "    (4, \"Will Johnson\", 1)\n",
    "], [\"id\", \"name\", \"city_id\"])\n",
    "\n",
    "# Broadcast Join\n",
    "joined_df = people.join(broadcast(cities), people.city_id == cities.city_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0454b80b-cfea-4bc0-b0be-49fbde87918e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-------+-------+-----------+\n",
      "| id|        name|city_id|city_id|  city_name|\n",
      "+---+------------+-------+-------+-----------+\n",
      "|  1|    John Doe|      1|      1|   New York|\n",
      "|  2|    Jane Doe|      2|      2|Los Angeles|\n",
      "|  3|   Sam Smith|      3|      3|    Chicago|\n",
      "|  4|Will Johnson|      1|      1|   New York|\n",
      "+---+------------+-------+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d90827-bb7a-4549-97d6-f58ee289c9f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
